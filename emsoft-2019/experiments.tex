%!TEX root = main.tex
\section{Experimental Results}
\input{table_pendulum_aircraft}
\input{table_integrator}

We evaluate a prototype implementation of the algorithm in~\autoref{lst:alg}
on three examples. For the first two, we apply the complete pipeline (design and memory optimization)
which returns an end-to-end robust controller. With the third example, we evaluate
the scalability of our approach when the number of regions and hyperplanes are in
the order of tens of thousands.

% This section reports experiments with the prototype described
% in~\autoref{lst:alg} applied to three different benchmarks: in the former two,
% we evaluate the complete pipeline (design and memory optimization) to produce in
% output a complete controller. In the last experiment, we want to show the
% scalability of our approach when the number of regions and hyperplanes are in
% the order of tens of thousands.

% For the design of controllers, we rely on MATLAB MPC toolbox. The outputs of the
% suite are vectors F, G and H, K for activation functions and hyperplanes
% equations. We encode both of them in Daisy for finite-precision optimization.
The design of end-to-end robust controllers has been performed on a laptop with Intel i7-6700HQ CPU at
2.60GHz, with 16GB of RAM. The evaluation of the last benchmark runs on a
cluster with 48 Intel Xeon v2 @ 3.00GHz cores with 1TB of RAM, of which our analysis
only used 15GB.
%Note that the memory consumption of our analysis never exceed 15GB of memory.

\subsection{End-to-end Robust Controller}
\eva{Use the same symbol for Delta and Eps as used in the technical section.}

We evaluate our complete pipeline on two benchmarks: the inverted pendulum
described in~\autoref{sec:example} and a well-known 4D example for aircraft
controller design.
\eva{Describe aircraft example, add citation}

For the pendulum problem, the domain of X is bounded in the range $X_{0} \in
(-3.23, 3.23)$ and $X_{1}\in (-1.46, 1.46)$. MATLAB designs a controller with 14
regions and 58 hyperplanes for each combination of delta and $\varepsilon$.
% delta spans the interval from 0.05 to 0.3, while $\varepsilon$ from 0.0006 to 0.0050.  % it's in the table
For the aircraft problem, the domain of X is bounded in the range $X_{0} \in
(-10000, 10000)$, $X_{1}\in (-734, 734)$, $X_{2}\in (-10000, 10000)$ and
$X_{3}\in (-790, 790)$. MATLAB returns a controller with 27 regions and 217
hyperplanes, except when delta=0.30 and $\varepsilon$=0.001 where the controller
has 26 regions and 208 hyperplanes. In general, when the disturbance value
increases, the state domain X shrinks to be robust against a greater
disturbance, or the number of regions reduces.

\eva{Explain why we choose 32 bits uniform as baseline}
\eva{Explain why we don't compare against other tools (resp. the one)}
\autoref{tab:ipd} shows the total number of bits required to implement each
controller using different precision options: uniform 32 bit precision
(`Uni32'), uniform but custom precision (`Uni', chosen precision in parentheses)
and mixed precision (`Mix'). We split the memory requirement into the bits
required for storing F and G and H and K. We show the results for each benchmark
for ten different combinations of delta and $\varepsilon$, varying one while
keeping the other fixed.
For the pendulum, the execution time of the analysis is 10 minutes no matter the
initial values for delta and $\varepsilon$, while for the aircraft it is 36
minutes. \rocco{maybe table for execution times}

% In \autoref{tab:ipd} we report the evaluation of our approach on both the
% inverted pendulum problem described in the motivation section, and a well-known
% 4D example for aircraft controller design. (Describe aircraft). For each
% benchmark, we report the results for ten different combinations of delta and
% $\varepsilon$. We divided them in two halves: in the  first, we fix the value
% for delta and we try different $\varepsilon$ combinations, while in the second
% halve we do the opposite.

% \eva{In the following, I'd discuss the two examples together and structure the 
% discussion as follows:
% \begin{itemize}
% 	\item give the constant info (number of regions etc.) for both benchmarks
% 	\item explain the variation of delta and eps (ideally with a motivation for why this is interesting)
% 	\item discuss the improvements we get (i.e. first discuss the metric we care about most)
% 	\item discuss other interesting details
% \end{itemize}
% Discussing the two examples together let's us compare their results easier, i.e.
% point out differences or commonalities.}

\eva{Fix Table 1 caption. It's not the difference, but savings. Difference is absolute.}
\eva{Compute the actual averages for all F, G, H, K and not only approximations}
For the inverted pendulum, minimal uniform precision saves ???\% of memory
compared to a uniform 32 bit baseline (column `\%32vU' in~\autoref{tab:ipd}) on
average overall (for F, G, H and K together). Mixed-precision further reduces the memory
requirement by ???\% on average with respect to the minimal uniform baseline
(column `\%UvM' in~\autoref{tab:ipd}). 
\autoref{tab:ipd} shows a more detailed breakdown of the memory requirement
between F and G and H and K. The memory requirements for the storage of
hyperplanes H and K depends only on the size of the tubes $\varepsilon$ so that
memory requirements remain constant for fixed $\varepsilon$.

\eva{Compute actual and overall averages here too}
For the aircraft example, minimal uniform precision saves on average ???\%
overall w.r.t. a uniform 32 bit baseline, and mixed-precision saves an
additional ???\% w.r.t. minimal uniform precision. We observe higher relative
memory savings by mixed-precision for storing H and K.

% we save 20\% (in average) of memory when activation functions
% are in minimal uniform precision, and an additional 7\% (in average) when we use
% mixed precision. For the storage of hyperplanes equations, we save more than
% 19\% of memory (in average) from uniform precision in 32bits to minimal uniform
% precision, and an additional 19\% (in average) from uniform to mixed precision.

%We noticed the following common behavior in the two benchmarks: 
As expected, when delta decreases, the activation functions (F, G) need to be
more precise and require more memory, because the space for approximation error
is reduced. Similarly, when the value of $\varepsilon$ increases, the memory
requirements for H, K can be relaxed.

We note two differences among these two benchmarks.
In the aircraft,
the precision for F and G is almost double with respect to the pendulum. This is
because the magnitude of F and G is on the order of $10^{3}$ while for the
pendulum it is on the order of several units (note that for F and G the memory gain
from Uni to Mix (\%UvM) is only slightly less than the one for the pendulum).

\eva{The below explanation only mentions aircraft. What is the difference to pendulum?}
The second difference consists in the last two lines in Table\ref{tab:ipd}.
In the aircraft, when $\varepsilon$=0.0030, the \maxUij exceeds delta=0.1 and we
need to design again the controller (it corresponds to the else branch in
Listing\ref{fig:overview}). The loop converges after 5 iterations (\texttt{then}
branch of the conditional) with delta=0.112. After each iteration, the new value
for delta is the last \maxUij\space plus a constant value in the order of
$10^{-3}$. Thanks to this $\varepsilon_{SAFE}$ we sensibly reduce memory demand
at the expense of slightly more disturbance for the controller. When
$\varepsilon$=0.0050 the analysis converges after 4 iterations and the same
discussion holds.


\eva{It is still not clear to me whether we need this discussion or the columns `max' and `$err(u_i)$'
in the table. These are internal computations and it seems that they do not really show anything
surprising/different than the number of bits show?}
The first halves of both pendulum and aircraft sections in \autoref{tab:ipd}, show how delta does not affect the maximal error among regions \maxUij. We verify that a variation for delta (typically in the order of $\pm$ 0.1) results in a minimal alteration to the domain of regions X (in the order of $\pm 10^{-10}$), not enough to produce a sensitive variation to \maxUij. Then, when \maxUij\space is constant the error bound for computations $err(u_{i})$ decreases together with delta. The memory required for the storage of F and G depends on $err(u_{i})$: the smaller the value for $err(u_{i})$, the more precise has to be the arithmetic for activation functions. 

In the second halves in \autoref{tab:ipd}, we fix the value for delta and we try different $\varepsilon$. When we increase the safe space between two regions by $\varepsilon$, the value for \maxUij is computed for new corner points. It happens because of the continuity of PWA activation functions: when \statevarmath is on the border between two generic regions $i$, $j$ the value for \maxUij is close to zero. The more $\varepsilon$ moves \statevarmath far from the border, the more \maxUij increases. When \maxUij is almost equal to delta, the space for $err(u_{i})$ is minimized, and the controller requires high precision for computations (see last lines for pendulum and aircraft in Table\ref{tab:ipd}). On the other hand, when $\varepsilon$ increases, the analysis can reduce the memory requirements for the storage of borders (see columns Uni and Mix in the second halves of both benchmarks).




\subsection{Scalability}
The benchmark in this experiment is the double integrator, a canonical example
of a second order control system. 
\eva{More details about this + citation}

For this example, we fix the values for Delta to 0.1 and $\varepsilon$ to 0.001,
but we consider also the prediction horizon m, as an input parameter for the
analysis. At design time, the prediction horizon of the controller m models how
many future iterations of the feed-forward system are considered in the
optimization problem (infinite horizon would correspond to the optimal
controller). Note that in this example, the number of regions is highly
correlated with the prediction horizon m of the controller: the number of regions
increases with larger m, and thus also increases the execution time of our analysis.
Even if this is a general behavior, it is not the rule.
\eva{What does the above sentence mean?}

The design of the controller in MATLAB takes few minutes, then controllers and
hyperplanes are encoded in Daisy for memory optimization. On average, the
analysis of a controller or an hyperplane in Daisy, requires less than two
minutes and less than 500MB of memory. The cluster used for the evaluation has
48 cores, and the memory consumption on the cluster is always bounded by
15GB. Memory optimization is trivially parallelizable, because any controller or
hyperplane can be analyzed independently.
\eva{There should be an explanation why this is not providing end-to-end robustness}


\autoref{tab:di} shows the details of this experiment.
We observe that our analysis scales up to 1200 controllers and 25000 hyperplanes in few
hours. 
\eva{Compute and say something about the overall savings for F, G, H, K, and proper average (at least for UvM)}
We further observe that relative savings remain largely constant for different 
prediction horizons.
% \autoref{tab:di} shows the stability of memory tuning in Daisy is
% constant with respect to the number of controllers or hyperplanes. This is true
% for both F, G and H, K. Indeed, the memory gain from uniform in 32bits and
% minimal uniform (\%32vU) is constant among different input values m. The same
% holds for (\%UvM).