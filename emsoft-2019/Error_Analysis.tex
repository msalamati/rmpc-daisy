%!TEX root = main.tex
\section{Error Analysis}
\label{sec:Error_Analysis}

In this section, we present our error analysis assuming that a fixed-point word lengths is given.
Then in the next section, we explain our optimization algorithm which determines suitable word
lengths fully automatically.% in~\autoref{sec:Controller_Synthesis}.
While we do not employ the error analysis independently of the controller and
precision synthesis, we note that it is an alternative way of bounding errors to
existing methods~\cite{imperialrmpc}, which phrase the roundoff error computation as a (relaxed)
mixed integer programming problem.
%\Sadegh{Alternative in what sense? What is the major difference between our analysis and \cite{imperialrmpc}?}

For simplicity of notation, we assume in this section a uniform word length $p$,
but our algorithm (and implementation) is applied to mixed-precision to reduce the memory usage.

Consider the controller obtained from explicit MPC according to Theorem~\ref{thm:EMPC}.
Define the affine control functions associated with each polyhedral region $\mathcal R_i$, $i\in\{1,2,\ldots, P\}$, as
$$v_{i}:\mathcal R_i\rightarrow \mathcal U \quad \text{ with } \quad v_{i}(x) := F_i x + G_i.$$
Implementation of the controller can be affected by two main sources of
error:
%which an error analysis has to capture \cite{imperialrmpc}:
\begin{itemize}
  \item[(i)] \emph{incorrect region selection}:
   instead of a correct affine function $v_{i}$, the implementation may choose an incorrect affine function $v_{j}$. This can happen either
    due to an analog-to-digital conversion in the measured states or due to the quantization in matrices $H_i$ and $K_i$ of the region $\mathcal R_i$; and
    
  \item[(ii)] \emph{approximation in the computation of the affine function}: for any selected region $\mathcal R_i$, the affine function $v_{i}$ is evaluated using the quantized versions of $F_i$ and $G_i$.
\end{itemize}

We have to ensure that the sum of these two errors remain below the bound $\Delta$
used in the design of the robust explicit MPC:
\begin{align}
  \max\{\|v_{i}-v_{j}\|,\,&\forall i,j,\,\, i\neq j, \mathcal{R}_{i}\cap\mathcal{R}_{j}\neq\emptyset\} \nonumber\\
  &+  \max\{err(v_{i})_{p},\, i=1,2,\ldots,P\} \le \Delta.
  \label{eq:delta}
\end{align}
%where $i,j$ are the indexes of two distinct regions $\mathcal{R}_{i}$ and $\mathcal{R}_{j}$, 
The first term is the error of incorrect region selection. The maximum is taken over all neighboring regions $\mathcal R_i,\mathcal R_j$ recognisable by having non-empty intersection. The function norm $\|v_{i}-v_{j}\|$ is taken by maximizing over all possible values of the state $x_k$. 
$err(v_{i})_{p}$ in
the second term captures the approximation error in the computation of the affine function $v_{i}$ when a fixed precision $p$ is used.
%$\mathit{neighbor}(\mathcal{R}_{i},\mathcal{R}_{j})$
%states that two regions share at least a point, $v_{i}$ is the control action
%for controller $i$, and $err(v_{i})_{p}$ captures the approximation error for
%computing the control action $v_{i}$ in fixed precision $p$.

\subsection{Incorrect Region Selection}

The first part of the error in~\autoref{eq:delta} captures the error due to
selecting of the incorrect region.
This would happen since the matrices $H_i,K_i$ are stored in the hardware with fixed-point formats. In order to quantify the error, we define the expanded border $\bar{\mathcal B}_{ij}$ as the tube around the border between the regions $\mathcal{R}_i$ and $\mathcal{R}_j$:
% \begin{equation*}
% \bar{\mathcal R}_i := \bigcup_{\hat H_i,\hat K_i} \left\{x\in\mathbb R^n\,|\,\hat H_i x\le \hat K_i, \|H_i-\hat H_i\|\le \epsilon_f,\|K_i-\hat K_i\|\le \epsilon_f\right\}
% \end{equation*}
% for $i\in\{1,2,\ldots, P\}$, where $\epsilon_f$ is the maximum error of using fixed-point format instead of the precise values. The norm is also applied element-wise to the entries of the matrices. 

\begin{equation*}
\hat{\mathcal B}_{ij} := \left\{x\in\mathbb R^n\,|\, \| H_i x - K_i\| \le \varepsilon \right\}
\end{equation*}
where $\varepsilon$ captures the uncertainty in computing the correct region.
%
The width of the tube $\varepsilon$ is due to two sources of errors:
analog-to-digital conversion $\varepsilon_{A/D}$ and the quantization of region
bounds in memory $\varepsilon_{Q}$:
\begin{equation}\label{eq:epsilontot}
  \varepsilon=\varepsilon_{A/D}+\varepsilon_{Q}
\end{equation}

Analog conversion happens just before the controller receives the sensor input from the plant
and is given by:
\begin{equation*}
\varepsilon_{A/D}=\frac{V_{cc}}{2^{r}-1}
\end{equation*}
where $V_{cc}$ is the reference voltage of the converter (e.g. typically 5V), $r$
is the number of bits available to quantize the analog signal, and $2^{r}$ is
the resolution of the converter.

While $\varepsilon_{A/D}$ is intrinsic to the capabilities of the device,
$\varepsilon_{Q}$ depends on the precision used to store the boundaries:
\begin{equation}\label{eq:quantizationlines}
  \varepsilon_{Q} = \|(H_i - \hat{H}_i) \hat{x} + (K_i - \hat{K}_i)\|
\end{equation}
where $\hat{x}$ is the finite-precision value of $x$.
%\eva{For which index is this evaluated, $i$, $j$ or both?}
That is, $\varepsilon_{Q}$ bounds the distance between any hyperplane in infinite
precision (H and K), and its counter-part quantized in $p$ bits precision
($\hat{H}_{p}$ and $\hat{K}_{p}$).
This second error can be tuned providing a trade-off between accuracy and
memory storage required.

% Note that $\mathcal R_i\subset \bar{\mathcal R}_i$. In other words, $\bar{\mathcal R}_i$ is the expanded version of $\mathcal R_i$.
% Note that $\bar{\mathcal R}_i\cap \bar{\mathcal R}_j$ gives us the states where one of the two regions $\mathcal R_i, \mathcal R_i$ might be selected instead of the other one because of the fixed-precision implementation of the regions.   
% $\bar{\mathcal R}_i$ is the expanded version of $\mathcal R_i$ that include all states in $\mathcal R_i$this definition expands the regions $\mathcal $
 %That is, with infinite-precision arithmetic
 Therefore, the first term in~\autoref{eq:delta} can be computed less conservatively by maximizing only over 
 the expanded borders:
 \begin{equation}
 \label{eq:maximization}
 \max\{\|F_ix+G_i-F_jx-G_j\|,\,x\in \bar{\mathcal B}_{ij}\}
% \max_{\forall i,j.\mathit{neighbor}(\mathcal{R}_{i},\mathcal{R}_{j})}&|F_{i}\statevar+G_{i} - (F_{j}\statevar+G_{j})|
 \end{equation} 
 
%and in the absence of any uncertainty from hardware measurements we would choose
%control action $v_{i}$, but due to uncertainties we actually choose control action $v_{j}$:
%\begin{align} \label{eq:maximization}
 % \max_{\forall i,j.\mathit{neighbor}(\mathcal{R}_{i},\mathcal{R}_{j})}|v_{i}-v_{j}|&= \\
 % \max_{\forall i,j.\mathit{neighbor}(\mathcal{R}_{i},\mathcal{R}_{j})}&|F_{i}\statevar+G_{i} - (F_{j}\statevar+G_{j})|\nonumber
%\end{align}

%Because of the linearity of the function $v_{i}-v_{j}$, and because of the
%convexity of the regions $i$ and $j$, it is enough to evaluate function
%(\ref{eq:maximization}) at the corner points of the \texttt{tube}, instead of
%solving a maximization problem.

%A naive computation of this error would consider all possible values of $x_k$.
%This would lead to a gross over-approximation, however, since the controller
%will choose a wrong region only in a relative narrow neighborhood, or
%\emph{tube}, around the boundary between two regions $R_i$ and $R_j$.

%The border between two regions $\mathcal{R}_{i}$ and $\mathcal{R}_{j}$ is defined by 
%their corresponding boundaries:
%\begin{equation}
%\mathit{border}_{i,j} :=(H_i \statevar - K_i ) - (H_j \statevar - K_j)
%\end{equation}
%for appropriate matching borders $H_i, K_i$ and $H_j, K_j$.

%In~\autoref{eq:maximization}, we thus constrain $\statevar$ to be within an appropriate tube:
%\begin{equation}\label{eq:tube}
%|\mathit{border}_{i,j}| \le \varepsilon
%\end{equation}
%where $\varepsilon$ captures the uncertainty in computing the
%correct region and thus the width of the tube.
%

\begin{comment}
We should also include the error in 
The width of the tube $\varepsilon$ is due to two sources of errors:
analog-to-digital conversion $\varepsilon_{A/D}$ and the quantization of region
bounds in memory $\varepsilon_{Q}$:
\begin{equation}\label{eq:epsilontot}
  \varepsilon=\varepsilon_{A/D}+\varepsilon_{Q}
\end{equation}

Analog conversion happens just before the controller receives the sensor input from the plant
and is given by:
\begin{equation*}
\varepsilon_{A/D}=\frac{V_{cc}}{2^{r}-1}
\end{equation*}
where $V_{cc}$ is the reference voltage of the converter (e.g. typically 5V), $r$
is the number of bits available to quantize the analog signal, and $2^{r}$ is
the resolution of the converter.

While $\varepsilon_{A/D}$ is intrinsic to the capabilities of the device,
$\varepsilon_{Q}$ depends on the precision used to store the boundaries:
\begin{equation}\label{eq:quantizationlines}
  \varepsilon_{Q} = |(H_i - \hat{H}_i) \qstatevar + (K_i - \hat{K}_i)|
\end{equation}
\eva{For which index is this evaluated, $i$, $j$ or both?}
That is, $\varepsilon_{Q}$ bounds the distance between any hyperplane in infinite
precision (H and K), and its counter-part quantized in $p$ bits precision
($\hat{H}_{p}$ and $\hat{K}_{p}$).
This second error can be tuned providing a trade-off between accuracy and
memory storage required.
\end{comment}


\subsection{Approximate Control Output}

% We should also include the error due to
% analog-to-digital conversion $\varepsilon_{A/D}$.
% Analog to digital conversion happens just before the controller receives the measured state from sensor
% and is given by:
% \begin{equation*}
% \varepsilon_{A/D}=\frac{V_{cc}}{2^{r}-1}
% \end{equation*}
% where $V_{cc}$ is the reference voltage of the converter (e.g. typically 5V), $r$
% is the number of bits available to quantize the analog signal, and $2^{r}$ is
% the resolution of the converter. The associated error in the output of the controller is $\varepsilon_{A/D}\max_i\{\|K+F_i\|\}$.

Once a quantized state $\hat x$ is given and a region $\mathcal R_i$ is chosen, computing $v_{i}$ itself introduces
imprecision, because, $v_{i}$ needs to be evaluated in finite-precision arithmetic:
\begin{equation}\label{eq:fperror}
  err(v_{i})_{p} = | (F_i - \hat{F}_{i}) \statevar + ( G_i - \hat{G}_{i} ) |
\end{equation}
where $\hat{F}$ and $\hat{G}$ represent the quantized values (in $p$ bits) for
the infinite precision values $F$ and $G$.

\autoref{eq:fperror} is evaluated for all \statevarmath that are inside the
region $\mathcal{R}_i$, together with all the values in the tube surrounding
region $\mathcal{R}_i$.
In this way we compute the error also for those points that belong to a neighbor
of $\mathcal{R}_i$, but because of finite-precision errors they are erroneously
mapped to control action $v_{i}$.

\subsection{Implementation}

The incorrect region error (\autoref{eq:maximization}) is computed with Matlab. 

Due to the affinity of function
$v_{i}-v_{j}$, and because we defined the tube as a convex region surrounding the corresponding hyperplane, it suffices
to evaluate the function only at the corner points of the tube, and keep the
result with the maximal magnitude. 
To evaluate the incorrect region selection error across
the corner points, we need to first locate the corner points as well as all the
regions which share those corner points. 
Each region $R_i$ is represented by a set of contraints. We first extract the
set of vertices of $R_i$ using the open source Matlab library
$\mathit{lcon2vert}$~\cite{??}.
For each computed vertex $v_i$, we compute a $n-D$
hypercube with the edge length of $2 \varepsilon$. 
Each of the $2^n$ vertices of this hypercube is counted as a corner point, on which
we evaluate $\|u_i - u_j\|$. 

In order to find out which regions share a vertex $v_i$, we implemented two approaches.
The first approach determines the set of neighboring regions via an exhaustive
search over the set of vertices. However, we observed that this algorithm only
scales to small numbers of regions.
Our second approach works on the observation that all the regions $R_j$s 
having $v_i$ as their vertex can be identified if at least $n$ of their hyperplanes cross $v_i$. 

% In brief, we go
% through these steps: (i) for each region, the set of vertices are computed; (ii)
% for the vertex $v_i$, the set of all regions $\mathcal R_{ij}$ having $v_i$ as
% their own vertex are found; (iii) for the vertex $v_i$, the set of corner points
% $v_{ik}$ are computed which form a hypercube centering at $v_i$; (iv) for every
% corner point $c_k=v_{ik}$ and for every pair $(\mathcal R_{ij1},\mathcal
% R_{ij_2})$, the corresponding corner error $|v_{j1}-v_{j2}|$ is computed; (v)
% finally, we pick the greatest corner error.

% \eva{This needs more details. Which function(s) and algorithm from Matlab are used?}
% \eva{How do you determine which regions are neighbors? resp. which borders are matching?}


$\varepsilon_Q$ and $err(v_{i})_{p}$ are computed by Daisy. For this we encode
the expressions in~\autoref{eq:quantizationlines} and \autoref{eq:fperror}
respectively as straight-line arithmetic expressions and specify the constraints
on the domain of $\statevar$ in the precondition.
Daisy performs a dataflow analysis to determine the finite-precision roundoff
errors. 
% This error computation differs from the one employed by
% \citet{imperialrmpc} which phrases the roundoff error computation as a (relaxed)
% mixed integer programming problem.

While our actual synthesis algorithm (\autoref{sec:Controller_Synthesis}) does
not compute the errors $\varepsilon_Q$ and $err(v_{i})_{p}$ explicitly, the
error verification is used internally by Daisy to determine a suitable precision
$p$ and can also be called explicitly without the precision optimization.

% The approximation error and the precision tuning for both borders and active
% functions are computed in Daisy. $\varepsilon_{Q}$ is given in input to the
% analysis. After we compute the incorrect region in MATLAB we obtain the value
% for max error $err(v_{i})$ from (8). Then we give the errors and the vectors F,G
% and H,K in input to Daisy. We used Daisy as a verification tool: we provide in
% input the expression we want to quantize (e.g. $\mathcal{R}_{i}$) together with
% the error bound we can tolerate (e.g. $\varepsilon_{Q}$), and Daisy returns a
% precision configuration (uniform or mixed) such that the input error is
% satisfied. The way it finds the right configuration for uniform precision is the
% following: it starts by checking whether the input expression, quantized in
% unary uniform precision (1bit), introduces an error bounded by the input error.
% If this is not the case, Daisy increases the precision format by 1 bit and check
% whether the error now is satisfied. This unary increment runs inside a loop that
% terminates when the first valid format is matched. This uniform word-length is
% going to be the baseline for the mixed-precision tuning phase. The
% mixed-precision algorithm keeps a list of variables to quantize, and collects
% different candidate configurations that satisfy the input error. It returns the
% best configuration based on the evaluation of a quality function.


